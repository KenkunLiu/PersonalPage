<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0030)https://pengsongyou.github.io/ -->
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kenkun Liu's Homepage</title>

  <meta name="keywords" content="Kenkun Liu,kenkun liu,kenkunliu,Liu Kenkun,Kenkun"/>
  <meta name="description" content="Kenkun Liu's Homepage"/>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE">
  <meta name="viewport" content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">

  <style type="text/css">
    @import url(http://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300,100italic,100);

    /* Color scheme stolen from Sergey Karayev */
    a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Arial', sans-serif;
      font-size: 15px;
      font-weight: 300;
    }

    strong {
      font-family: 'Arial', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 600;
    }

    heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Arial', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 200;
    }

    papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Arial', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight: 300;
    }

    name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Arial', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 140px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <!--<tr onmouseout="headshot_stop()" onmouseover="headshot_start()">-->
            <tbody>
              <tr>
                <td width="67%" valign="middle">
                  <p align="center">
                    <name>Kenkun Liu (ÂàòÂû¶Âù§)</name>
                  </p>

                  <p>
                    I am currently a PhD student at <a href="https://www.cuhk.edu.cn/en"> The Chinese University of Hong Kong, Shenzhen</a>, under the
                    supervision of <a href="https://gaplab.cuhk.edu.cn/pages/people">Prof. Xiaoguang Han</a>. I am now a research intern at StepFun, studying multi-modal foundation model.
                  </p>
                  <p>
                    My past research interests are mainly focused on Human-centric Computer Vision, including 2D/3D pose and shape estimation, human body rendering. Now I focus on multi-modal foundation model, especially on unified visual understanding and generation, visual reasoning.
                  </p>

                  <p align="center">
                    <a href="mailto:liu.kenkun33@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=qy40HA0AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/KenkunLiu">Github</a>
                  </p>
                </td>

                <td width="33%">
                  <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/me.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Internships -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Internships</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/step.png' width="100">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <!-- <a> -->
                  <p> StepFun (Èò∂Ë∑ÉÊòüËæ∞).</p> 
                  <!-- </a> -->
                  <p>
                    Time: 2025.04-Now </p> Mentors: ËëõÊîøÔºåÂ≠ôÊ≥â
                  </p>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/tongyi.png' width="100">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <!-- <a> -->
                  <p>Tongyi Lab (ÈÄö‰πâÂÆûÈ™åÂÆ§), Alibaba Group.</p> 
                  <!-- </a> -->
                  <p>
                    Time: 2024.07-2025.04 </p> Mentors: È°æÂ∞è‰∏úÔºåÂéüÁéÆÊµ©ÔºåÈÇ±ÈôµËÖæ
                  </p>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/idea.png' width="125">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <!-- <a> -->
                  <p>IDEA (International Digital Economy Academy), Á≤§Ê∏ØÊæ≥Â§ßÊπæÂå∫Êï∞Â≠óÁªèÊµéÁ†îÁ©∂Èô¢</p> 
                  <!-- </a> -->
                  <p>
                    Time: 2023.07-2024.05 </p> Mentors: Âº†Á£äÔºåÊõæÁà±Áé≤ÔºåËíãÂ∞èÂèØ 
                  </p> 

              </tr>
            </tbody>
          </table>


          <!-- Researchs -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/UniG.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://arxiv.org/abs/2404.16323">
                    <papertitle>UniG: Modelling Unitary 3D Gaussians for View-Consistent 3D Reconstruction</papertitle>
                  </a>
                  <br>
                  <em>Jiamin Wu*</em>,
                  <strong>Kenkun Liu*</strong>,
                  <em>Yukai Shi</em>,
                  <em>Xiaoke Jiang</em>,
                  <em>Yuan Yao</em>,
                  <em>Lei Zhang</em>
                  <br>
                  <em>arXiv:2410.13195</em>
                  <br>
                  <a href="https://arxiv.org/pdf/2410.13195">[Paper] </a>
                  <a href="https://github.com/jwubz123/UNIG"> [Code] </a>
                  <a href="https://kenkunliu.github.io/UniG/">[Project Page] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/MaQ.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a >
                    <papertitle>Motions as Queries: One-Stage Multi-Person Holistic Human Motion Capture</papertitle>
                  </a>
                  <br>
                  <strong>Kenkun Liu*</strong>,
                  <em>Yurong Fu*</em>,
                  <em>Weihao Yuan*</em>,
                  <em>Jing Lin</em>,
                  <em>Peihao Li</em>,
                  <em>Xiaodong Gu</em>,
                  <em>Lingteng Qiu</em>,
                  <em>Haoqian Wang</em>,
                  <em>Zilong Dong</em>,
                  <em>Xiaoguang Han</em>,
                  <br>
                  <em>CVPR2025</em>
                  <br>
                  <a>[Paper] </a>
                  <a> [Code] </a>
                  <a>[Project Page] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/DIG3D.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://arxiv.org/abs/2404.16323">
                    <papertitle>LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling 3D Gaussians</papertitle>
                  </a>
                  <br>
                  <em>Jiamin Wu*</em>,
                  <strong>Kenkun Liu*</strong>,
                  <em>Han Gao</em>,
                  <em>Xiaoke Jiang</em>,
                  <em>Lei Zhang</em>
                  <br>
                  <em>CVPR2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2404.16323">[Paper] </a>
                  <a> [Code] </a>
                  <a href="https://kenkunliu.github.io/DIG3D/">[Project Page] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/MVHumanNet.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://arxiv.org/pdf/2312.02963.pdf">
                    <papertitle>MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</papertitle>
                  </a>
                  <br>
                  <em>Zhangyang Xiong*</em>,
                  <em>Chenghong Li*</em>,
                  <strong>Kenkun Liu*</strong>,
                  <em>Hongjie Liao</em>,
                  <em>Jianqiao Hu</em>,
                  <em>Junyi Zhu</em>,
                  <em>Shuliang Ning</em>,
                  <em>Lingteng Qiu</em>,
                  <em>Chongjie Wang</em>,
                  <em>Shijie Wang</em>,
                  <em>Shuguang Cui</em>,
                  <em>Xiaoguang Han</em>
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2312.02963.pdf">[Paper] </a>
                  <a href="https://github.com/GAP-LAB-CUHK-SZ/MVHumanNet">[Code] </a>
                  <a href="https://x-zhangyang.github.io/MVHumanNet/">[Project Page] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/HMNeRFBench.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://openreview.net/pdf?id=7kc4gtEk3b">
                    <papertitle>HMNeRFBench: A Comprehensive Benchmark for Neural Human Radiance Fields</papertitle>
                  </a>
                  <br>
                  <strong>Kenkun Liu</strong>,
                  <em>Derong Jin</em>,
                  <em>Ailing Zeng</em>,
                  <em>Xiaoguang Han</em>,
                  <em>Lei Zhang</em>
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://openreview.net/pdf?id=7kc4gtEk3b">[Paper] </a>
                  <a href="https://github.com/KenkunLiu/HMNeRFBench_release/">[Code] </a>
                  <a href="https://kenkunliu.github.io/HMNeRFBench/">[Project Page] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>

          
          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/Mutual_Guidance.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://arxiv.org/pdf/2211.13919.pdf">
                    <papertitle>Mutual Guidance and Residual Integration for Image Enhancement</papertitle>
                  </a>
                  <br>
                  <em>Kun Zhou</em>,
                  <strong>Kenkun Liu</strong>,
                  <em>Wenbo Li</em>,
                  <em>Xiaoguang Han</em>,
                  <em>Jiangbo Lu</em>
                  <br>
                  <em>arXiv:2211.13919</em>
                  <br>
                  <a href="https://arxiv.org/pdf/2211.13919.pdf">[Paper] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table> -->

<!-- 
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/ETH_Seg.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Qiu_ETHSeg_An_Amodel_Instance_Segmentation_Network_and_a_Real-World_Dataset_CVPR_2022_paper.pdf">
                    <papertitle>ETHSeg: An Amodel Instance Segmentation Network and a Real-world Dataset for X-Ray Waste Inspection</papertitle>
                  </a>
                  <br>
                  <em>Lingteng Qiu</em>,
                  <em>Zhangyang Xiong</em>,
                  <em>Xuhao Wang</em>,
                  <strong>Kenkun Liu</strong>,
                  <em>Yihan Li</em>,
                  <em>Guanying Chen</em>,
                  <em>Xiaoguang Han</em>,
                  <em>Shuguang Cui</em>
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Qiu_ETHSeg_An_Amodel_Instance_Segmentation_Network_and_a_Real-World_Dataset_CVPR_2022_paper.pdf">[Paper] </a>
                  <a href="https://github.com/GAP-LAB-CUHK-SZ/ETHSeg">[Code] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table> -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/global_pose.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Liu_Learning_Global_Pose_Features_in_Graph_Convolutional_Networks_for_3D_ACCV_2020_paper.pdf">
                    <papertitle>Learning Global Pose Features in Graph Convolutional Networks for 3D Human Pose Estimation</papertitle>
                  </a>
                  <br>
                  <strong>Kenkun Liu</strong>,
                  <em>Zhiming Zou</em>,
                  <em>Wei Tang</em>
                  <br>
                  <em>ACCV</em>, 2020
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Liu_Learning_Global_Pose_Features_in_Graph_Convolutional_Networks_for_3D_ACCV_2020_paper.pdf">[Paper] </a>
                  <!-- <a href="https://github.com/UX-Decoder/DINOv">[Code] </a> -->
                  <br>
                </td>
              </tr>
            </tbody>
          </table>
<!-- 
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/high_order.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://www.bmvc2020-conference.com/assets/papers/0550.pdf">
                    <papertitle>High-order Graph Convolutional Networks for 3D Human Pose Estimation</papertitle>
                  </a>
                  <br>
                  <em>Zhiming Zou</em>,
                  <strong>Kenkun Liu</strong>,
                  <em>Le Wang</em>,
                  <em>Wei Tang</em>
                  <br>
                  <em>BMVC</em>, 2020
                  <br>
                  <a href="https://www.bmvc2020-conference.com/assets/papers/0550.pdf">[Paper] </a>
                  <br>
                </td>
              </tr>
            </tbody>
          </table> -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
                <td width="25%">
                  <div class="one">
                    <img src='images/weight_sharing_mechanisms.png' width="150">
                  </div>
                </td>
                <td valign="top" width="75%">
                  <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550324.pdf">
                    <papertitle>A Comprehensive Study of Weight Sharing in Graph Networks for 3D Human Pose Estimation</papertitle>
                  </a>
                  <br>
                  <strong>Kenkun Liu*</strong>,
                  <em>Rongqi Ding*</em>,
                  <em>Zhiming Zou</em>,
                  <em>Le Wang</em>,
                  <em>Wei Tang</em>
                  <br>
                  <em>ECCV</em>, 2020
                  <br>
                  <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550324.pdf">[Paper] </a>
                  <!-- <a href="https://github.com/UX-Decoder/DINOv">[Code] </a> -->
                  <br>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Services -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Services</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- <ul>
            <li>
                <strong>Chinese National Scholarship</strong>
                , Ministry of Education of P.R. China
            </li>
            <li>Huawei Scholarship, Huawei Technologies Co., Ltd</li>
            <li>First Prize Scholarship, Xidian University</li>
            <li>First Prize, China Undergraduate Mathematical Contest in Modeling (CUMCM)</li>
            <li>Finalist Winner (First prize), International Mathematical Contest in Modeling (MCM/ICM)</li>
            <li>First Prize in Jiangsu Province, Chinese Physics Olympiad (Rank 66)</li>
        </ul> -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
            <tr>
            <td>
              <p >Reviewer: IJCV, NeurIPS, CVPR, ICCV, ECCV</p>
            </td>
            </tr>
          </tbody>
        </table>
        


</body>

</html>
